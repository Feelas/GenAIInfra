apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
labels:
  opendatahub.io/dashboard: "true"
metadata:
  annotations:
    openshift.io/display-name: ise-uiuc/Magicoder-S-DS-6.7B on CPU
  name: tgi-on-cpu-magicoder
spec:
  containers:
    - args:
        - --model-id
        - /mnt/models/--ise-uiuc--Magicoder-S-DS-6.7B/snapshots/b3ed7cb1578a3643ceaf2ebf996a3d8e85f75d8f
        - --port=8080
        - --json-output
      env:
        - name: NUMBA_CACHE_DIR
          value: /tmp/hf_home
        - name: HF_HOME
          value: /tmp/hf_home
        - name: HF_HUB_CACHE
          value: /mnt/models
        - name: HUGGING_FACE_HUB_TOKEN
          value: "{{ .Values.huggingfacehubApiToken }}"
        - name: BATCH_BUCKET_SIZE
          value: "22"
        - name: PREFILL_BATCH_BUCKET_SIZE
          value: "1"
        - name: MAX_BATCH_PREFILL_TOKENS
          value: "5102"
        - name: MAX_BATCH_TOTAL_TOKENS
          value: "32256"
        - name: MAX_INPUT_LENGTH
          value: "1024"
        - name: PAD_SEQUENCE_TO_MULTIPLE_OF
          value: "1024"
        - name: MAX_WAITING_TOKENS
          value: "5"
        - name: OMPI_MCA_btl_vader_single_copy_mechanism
          value: none
      image: ghcr.io/huggingface/text-generation-inference:1.4
      livenessProbe:
        exec:
          command:
            - curl
            - localhost:8080/health
        initialDelaySeconds: 500
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
      readinessProbe:
        exec:
          command:
            - curl
            - localhost:8080/health
        initialDelaySeconds: 500
      volumeMounts:
        - mountPath: /data
          name: model-volume
  volumes:
    - name: model-volume
      emptyDir:
        sizeLimit: 300Gi
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 40Gi
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: llm
